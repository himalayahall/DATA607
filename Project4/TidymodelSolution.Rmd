---
title: "Project 4 - text classification"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this markdown we will use [tidymodels](https://www.tidymodels.org) to build ham/spam classifiers for SMS/Email messages. The library provides a framework for modeling and machine learning using [tidyverse](https://www.tidyverse.org) principles.

# Load library

```{r load-libs}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(textrecipes)
library(readtext)
```

```{r set-seed}
set.seed(1234)
```

```{r deletec-cpu-cores}
parallel::detectCores(logical = TRUE)
```

On Unix and Mac we can leverage multiple cpu/core. Comment out on Windows.

```{r unix-and-mac-only}
library(doMC)
registerDoMC(cores = 2)
```

# Load data

We have two data sets are available for building classifiers. One is a collection of SMS messages, the other is a (much larger) collection of emails. SMS messages are shorter and less noisy. Email messages are longer and contain significant noise (Html tags, email headers, etc.).

```{r load-data}
# df <- read.delim("https://raw.githubusercontent.com/himalayahall/DATA607/main/Project4/SMSSpamCollection.txt",
#                   sep = '\t',
#                   col.names = c("category", "text"),
#                   quote = "")
# df <- df %>% 
#         mutate(subject = '') %>%
#         mutate(from = '')
        
# Email data  is too big to load from [Github](https://github.com/himalayahall/DATA607/blob/main/Project4/EMAILSpamCollection.csv). Download into current 
df <- read.csv("./EMAILSpamCollection.csv")

df <- df %>% 
    filter(category != "") %>% 
    filter(text != "")

glimpse(df)
```

We expect data sets to be unbalanced, i.e. number of spam observations is a minority class. This class imbalance will influence how we build ML models.

```{r props}
props <- df %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))
props
```

Mutate category into a factor and add word count column.

```{r factorize-catrgory}
df <- df %>%
    mutate(category = as.factor(category)) %>%
    mutate(n_words = tokenizers::count_words(text))
    
glimpse(df)
```

Take look at the distribution of number of words per observation.

```{r}
summary(df$n_words)
```

## Base model

Split into train and test data sets.

```{r create-data-splits}
data_split <- initial_split(df, strata = category)
data_train <- training(data_split)
data_test <- testing(data_split)
```

Verify train/test splits have the same proportions as original data set. As we see, the training and test splits have the same proportion as original.

```{r}
# training set proportions by category
data_train %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))

# test set proportions by category
data_test  %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))
```

Set threshold for maximum unique words in data set.

```{r}
max_words <- 10000
```

Set up feature engineering recipe to get the data ready mo modeling. We set it up to predict **category** (outcome) from the following predictors - email **subject**, sender email address (**from**), and the email **text**.  

Predictors are tokenized, stop words are removed, tokens are stemmed, and filtered (max number of tokens retained and min times a token must appear in corpus in order to be considered). Finally, tokens are encoded using **tf-idf**. 

Knowing that we have a large quantity of data but the data is unbalanced, we down-sample such that the majority class level will have (at most) twice as many rows as the minority level.  

Token transformations are done via [textrecipes] (https://www.rdocumentation.org/packages/textrecipes/versions/1.0.10)

```{r create-model-recipe}
#library(themis)
data_recipe <- recipe(
                category ~ from + subject + text, data = data_train) %>%
                step_tokenize(text, 
                              options = list(lowercase = TRUE, 
                                             strip_punct = TRUE)) %>%
                step_stopwords(text) %>%
                step_stem(text) %>%
                step_tokenfilter(text, 
                                 min_times = .01* NROW(data_train), # must be in 1% corpus
                                 max_tokens = max_words
                                )  %>%
                step_tfidf(text) %>%
    
                step_tokenize(from, 
                              options = list(lowercase = TRUE, 
                                             strip_punct = TRUE)) %>%
                step_tfidf(from) %>%
    
                step_tokenize(subject, 
                              options = list(lowercase = TRUE, 
                                             strip_punct = TRUE)) %>%
                step_tfidf(subject) %>%

                step_downsample(category, 
                                under_ratio = 2 
                                )
data_recipe
```

To evaluate the model we will use 10-fold cross-validation. Make sure the folds have the same strata as the original data set.

```{r}
data_folds <- vfold_cv(data_train, strata = category)
data_folds
```

## Null model

Before building classification models it's important to look at the null model. For binary classification the null model always predicts the majority class. The **accuracy** of the null model will equal the proportion of the majority class. For example, if there are 90% hams (not spam) observations in the data set then accuracy of the null model will also be 90%.  

```{r}
prep(data_recipe, verbose = TRUE)
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(null_classification) %>%
  fit_resamples(
    data_folds, 
    metrics = metric_set(accuracy, roc_auc, precision, recall),
    control = control_resamples(save_pred = TRUE)
  )
```

The null model has high **accuracy** (proportion of the majority class) and a terrible [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). It has a high [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and an abysimal [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

```{r}
null_rs %>%
  collect_metrics()
null_rs
```

As expected, the null model simply guesses the majority class *ham* on each prediction. ROC 

```{r}
conf_mat_resampled(null_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")

```

The [tidymodels](https://tidymodels.tidymodels.org) framework makes it easy to plug-and-play with models. Below are some models that we can use - un-comment to select.

Random forest.

```{r models}
model <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine(engine = "ranger", importance = "impurity")

# Linear support vector machine.
# model <- svm_linear() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "kernlab")

# Boosted trees.
# model <- boost_tree() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "xgboost", trees = 20)
# model

# Linear support vector machine.
# model <- svm_linear() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "kernlab")
# model

# Multi-layer perceptron, a single layer neural network.
# num_epochs = 50 # for SMS data set
# num_epochs = 50 # for Email data set
# model <- mlp(hidden_units = 128, dropout = 0.1, epochs = num_epochs) %>%
#   set_mode("classification") %>%
#   # Also set engine-specific `verbose` argument to prevent logging the results:
#   set_engine("keras", verbose = 1)

model
```

Create [workflow](https://workflows.tidymodels.org) using above recipe and model. A workflow wraps the data recipe and model.

```{r}
data_wf <- workflow() %>%
            add_recipe(data_recipe) %>%
            add_model(model)
data_wf
```

Define metrics of interest.

```{r}
metrics = metric_set(accuracy, roc_auc, pr_auc, precision, recall)
metrics
```

Train the model on folds, record desired metrics, save predictions.

```{r}
nb_rs <- fit_resamples(
  data_wf,
  data_folds,
  metrics = metrics,
  control = control_resamples(save_pred = TRUE)
)
```

Extract metrics and predictions.

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics
```

Plot ROC curve for all folds.

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = category, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "Each resample fold is shown in a different color"
  )
```

Confusion matrix.

```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
```

## Final model

Final fit (with all training data).

```{r}
final_fitted <- last_fit(data_wf, data_split, metrics = metrics)
```

Extract predictions and metrics. The ROC is above 95% which is indicative of a good model.

```{r}
final_metrics <- collect_metrics(final_fitted)
final_predictions <- collect_predictions(final_fitted)

final_metrics
```

The confusion matrix confirms the model is performing well.

```{r}
final_predictions %>%
  conf_mat(truth = category, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Plot final model's ROC curve.

```{r}
final_predictions  %>%
  roc_curve(truth = category, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "With final random forest classifier on the training set"
  )
```
