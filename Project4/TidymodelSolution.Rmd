---
title: "Project 4"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  pdf_document: 
    toc: true
    number_sections: true
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r load-libs}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(textrecipes)
library(readtext)

```

```{r set-seed}
set.seed(1234)
```

```{r}
parallel::detectCores(logical = TRUE)
```

On Unix and Mac we can leverage multiple cpu/core. Comment out on Windows.

```{r unix-and-mac-onlu}
library(doMC)
registerDoMC(cores = 2)
```

# Load data

```{r load-data}
df <- read.delim("SMSSpamCollection.txt", 
                  sep = '\t', 
                  col.names = c('cat', 'sms'), 
                  quote = "")
glimpse(df)
```

We expect the SMS data to be  unbalanced, i.e. number of spam observations is the minority class. This is validated by noting that proportion of spam observations is 13.4%. 

```{r}
df %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
```

Make cat (category) a factor type and add a word count column, n_words.

```{r}
df <- df %>%
    mutate(cat = as.factor(cat)) %>%
    mutate(n_words = tokenizers::count_words(sms))
    
glimpse(df)
```

Look at the proportion. Notice that spam observations are in minority. This class imbalance will have an impact of how we build  ML models.

```{r}
df %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
```

## Base model

Split into train and test data sets.

```{r}
sms_split <- initial_split(df, strata = cat)
sms_train <- training(sms_split)
sms_test <- testing(sms_split)
```

Verify train/test splits have the same proportions as original data set. As we see, the training and test splits have the same proportion as original.

```{r}
# training set proportions by category
sms_train %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))

# test set proportions by category
sms_test  %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
```

Let's take a look at the word count distribution. Most SMS observations are between 10-40 words.

```{r}
sms_train %>%
  mutate(n_words = tokenizers::count_words(sms)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(x = "Number of words per sms",
       y = "Number of sms")
```

Set threshhold for maximum unique words in data set.

```{r}
max_words <- 10000
```

Set up recipe. We set it up to predict **cat** (outcome) from **sms** (predictor). The predictor is tokenized, stopwords are removed, and tokens are filtered. Tokens are encoded using **tf-idf**. Finally, data is downsampled such that the majority levels will have (at most) (approximately) twice as many rows than the minority level.

```{r}
library(themis)
sms_recipe <- recipe(
                cat ~ sms, data = sms_train) %>%
                step_tokenize(sms, 
                              options = list(lowercase = TRUE, 
                                             strip_punct = TRUE)) %>%
                step_stopwords(sms) %>%
                step_tokenfilter(sms, 
                                 max_tokens = max_words, 
                                 min_times = 10
                                )  %>%
                step_tfidf(sms) %>%
                step_downsample(cat, 
                                under_ratio = 2 
                                )
sms_recipe
```

There are a number of models available. We will select **random forest**.

```{r}
library(discrim)
model <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine(engine = "ranger", importance = "impurity")
model
```

Create workflow with recipe and model.

```{r}
sms_wf <- workflow() %>%
            add_recipe(sms_recipe) %>%
            add_model(model)
sms_wf
```

We will use 10-fold cross validation to train and evaluate our model. Also make sure that the folds have the same strata as the original data set.

```{r}
sms_folds <- vfold_cv(sms_train, strata = cat)
sms_folds
```

Train the model, record key metrics, save predictions.

```{r}
nb_rs <- fit_resamples(
  sms_wf,
  sms_folds,
  metrics = metric_set(accuracy, roc_auc, precision, recall),
  control = control_resamples(save_pred = TRUE)
)
```

Extract metrics and predictions.

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics
```

Plot ROC curve for all folds.

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = cat, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for SMS (ham)",
    subtitle = "Each resample fold is shown in a different color"
  )
```

Confusion matrix.

```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
```

## Compare to Null model

```{r}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(sms_recipe) %>%
  add_model(null_classification) %>%
  fit_resamples(
    sms_folds, 
    metrics = metric_set(accuracy, roc_auc, precision, recall),
    control = control_resamples(save_pred = TRUE)
  )
```

The null model has a high **accuracy** (same as proportion of the majority class) and a terrible **roc_auc**.

```{r}
null_rs %>%
  collect_metrics()
```

As expected, the null model simply guesses the majority class *ham* on each prediction.

```{r}
conf_mat_resampled(null_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")

```

## Final model

Final fit (with all training data).

```{r}
final_fitted <- last_fit(sms_wf, sms_split)
```

Extract predictions and metrics. Final model looks quite good with a **roc_auc** of 0.981.

```{r}
final_metrics <- collect_metrics(final_fitted)
final_predictions <- collect_predictions(final_fitted)

final_metrics
```

The confusion matrix confirms the model is performing well.

```{r}
final_predictions %>%
  conf_mat(truth = cat, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Plat final model's ROC curve.

```{r}
final_predictions  %>%
  roc_curve(truth = cat, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for SMS (ham)",
    subtitle = "With final random forest classifier on the training set"
  )
```


