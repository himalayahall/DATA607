---
title: "Project 4 - text classification"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this markdown we will use [tidymodels](https://www.tidymodels.org) to build ham/spam classifiers for SMS/Email messages. The library provides a framework for modeling and machine learning using [tidyverse](https://www.tidyverse.org) principles.

# Load library

```{r load-libs}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(textrecipes)
library(readtext)
```

```{r set-seed}
set.seed(1234)
```

```{r deletec-cpu-cores}
parallel::detectCores(logical = TRUE)
```

On Unix and Mac we can leverage multiple cpu/core. Comment out on Windows.

```{r unix-and-mac-only}
library(doMC)
registerDoMC(cores = 2)
```

# Load data

There are 2 data sets we can use for modeling. One is a collection of SMS messages, the other is a (much larger) collection of emails. SMS messages are shorter and less noisy. Email messages are longer and contain significant noise (Html tags, email headers, etc.).

```{r load-data}
# df <- read.delim("SMSSpamCollection.txt", 
#                   sep = '\t', 
#                   col.names = c('cat', 'text'), 
#                   quote = "")
df <- read.csv("./EMAILSpamCollection.csv")
df <- df %>% filter(cat != "")
glimpse(df)
```

We expect the data to be unbalanced, i.e. number of spam observations is a minority class.

```{r props}
props <- df %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
props
```

Make cat (category) a factor type and add a word count column, n_words.

```{r factorize-cat}
df <- df %>%
    mutate(cat = as.factor(cat)) %>%
    mutate(n_words = tokenizers::count_words(text))
    
glimpse(df)
```

Look at the proportion. Notice that spam observations are in minority. This class imbalance will have an impact of how we build ML models.

```{r}
df %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
```

## Base model

Split into train and test data sets.

```{r create-data-splits}
data_split <- initial_split(df, strata = cat)
data_train <- training(data_split)
data_test <- testing(data_split)
```

Verify train/test splits have the same proportions as original data set. As we see, the training and test splits have the same proportion as original.

```{r}
# training set proportions by category
data_train %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))

# test set proportions by category
data_test  %>% 
  count(cat) %>% 
  mutate(prop = n/sum(n))
```

Let's take a look at the word count distribution. Most observations conmtain between 10-40 words.

```{r}
data_train %>%
  mutate(n_words = tokenizers::count_words(text)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(x = "Number of words per observation",
       y = "Number of observations")
```

Set threshold for maximum unique words in data set.

```{r}
max_words <- 10000
```

Set up feature engineering recipe to get the data ready mo modeling. We set it up to predict **cat** (outcome) from **text** (predictor). The predictor is tokenized, stop words are removed, and tokens are filtered (max number of tokens retained and min times a token must appear to be considered at all). Tokens are encoded using **tf-idf**. Finally, data is down-sampled such that the majority class level will have (at most) twice as many rows as the minority level.

```{r create-model-recipe}
library(themis)
data_recipe <- recipe(
                cat ~ text, data = data_train) %>%
                step_tokenize(text, 
                              options = list(lowercase = TRUE, 
                                             strip_punct = TRUE)) %>%
                step_stopwords(text) %>%
                step_tokenfilter(text, 
                                 max_tokens = max_words, 
                                 min_times = 10
                                )  %>%
                step_tfidf(text) %>%
                step_downsample(cat, 
                                under_ratio = 2 
                                )
data_recipe
```

There is a wide palette of models The [tidymodels](https://tidymodels.tidymodels.org) framework makes it easy to plug-and-play with models. Below are 2 models, **mlp** (multi-layer perceptron, a single layer neural network) and **random forest**.

```{r}
# model <-
#   mlp(hidden_units = 256, dropout = 0.1, epochs = 20) %>%
#   set_mode("classification") %>%
#   # Also set engine-specific `verbose` argument to prevent logging the results:
#   set_engine("keras", verbose = 1)
```

Use random forest model.

```{r}
library(discrim)
model <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine(engine = "ranger", importance = "impurity")
model
```

Create [workflow](https://workflows.tidymodels.org) using above recipe and model.

```{r}
data_wf <- workflow() %>%
            add_recipe(data_recipe) %>%
            add_model(model)
data_wf
```

We will use 10-fold cross validation to train and evaluate our model. Also make sure that the folds have the same strata as the original data set.

```{r}
data_folds <- vfold_cv(data_train, strata = cat)
data_folds
```

Train the model, record key metrics, save predictions.

```{r}
nb_rs <- fit_resamples(
  data_wf,
  data_folds,
  metrics = metric_set(accuracy, roc_auc, precision, recall),
  control = control_resamples(save_pred = TRUE)
)
```

Extract metrics and predictions.

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics
```

Plot ROC curve for all folds.

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = cat, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "Each resample fold is shown in a different color"
  )
```

Confusion matrix.

```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
```

## Compare to Null model

```{r}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(null_classification) %>%
  fit_resamples(
    data_folds, 
    metrics = metric_set(accuracy, roc_auc, precision, recall),
    control = control_resamples(save_pred = TRUE)
  )
```

The null model has a high **accuracy** (same as proportion of the majority class) and a terrible **roc_auc**.

```{r}
null_rs %>%
  collect_metrics()
```

As expected, the null model simply guesses the majority class *ham* on each prediction.

```{r}
conf_mat_resampled(null_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")

```

## Final model

Final fit (with all training data).

```{r}
final_fitted <- last_fit(data_wf, data_split)
```

Extract predictions and metrics. Final model looks quite good with a **roc_auc** of 0.981.

```{r}
final_metrics <- collect_metrics(final_fitted)
final_predictions <- collect_predictions(final_fitted)

final_metrics
```

The confusion matrix confirms the model is performing well.

```{r}
final_predictions %>%
  conf_mat(truth = cat, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Plat final model's ROC curve.

```{r}
final_predictions  %>%
  roc_curve(truth = cat, .pred_ham) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "With final random forest classifier on the training set"
  )
```
