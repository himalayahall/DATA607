---
title: "Project 4 - text classification"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(timeit = local({
  now = NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res = difftime(Sys.time(), now)
      now <<- NULL
      # use options$label if you want the chunk label as well
      paste('Time for this code chunk:', as.character(res))
    }
  }})
)
```

# Introduction

In this markdown we will use [tidymodels](https://www.tidymodels.org) to build ham/spam classifiers for SMS/Email messages. The library provides a framework for modeling and machine learning using [tidyverse](https://www.tidyverse.org) principles.

# Load library

```{r load-libs}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(textrecipes)
library(readtext)
```

```{r set-seed}
set.seed(1234)
```

Let's detect the number of available cores. We use this during model setup.

```{r detect-cpu-cores}
detected_cores <- parallel::detectCores(logical = TRUE)
usable_cores = 1 + (detected_cores / 2)
usable_cores
```

On Unix and Mac we can leverage multiple CPU/core. Comment out on Windows.

```{r unix-and-mac-only}
library(doMC)
registerDoMC(cores = usable_cores)
```

# Load data

We have two data sets are available for building classifiers. One is a collection of SMS messages, the other is a (much larger) collection of emails. SMS messages are shorter and less noisy. Email messages are longer and contain significant noise (Html tags, email headers, etc.).

```{r load-data}
# df <- read.delim("https://raw.githubusercontent.com/himalayahall/DATA607/main/Project4/SMSSpamCollection.txt",
#                   sep = '\t',
#                   col.names = c("category", "text"),
#                   quote = "")
# df <- df %>% 
#         mutate(subject = '') %>%
#         mutate(from = '')
        
# Email data  is too big to load from [Github](https://github.com/himalayahall/DATA607/blob/main/Project4/EMAILSpamCollectionFull.csv). Download into current 
df <- read.csv("./EMAILSpamCollectionFull.csv")

df <- df %>% 
    filter(text != "") %>%
    mutate(category = factor(category, levels = c('spam', 'ham'))) %>% # order is important - spam first
    mutate(n_words = tokenizers::count_words(text))

glimpse(df)
```

We expect data sets to be unbalanced, i.e. number of spam observations is a minority class. This class imbalance will influence how we build ML models.

```{r props}
props <- df %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))
props
```

## Data split

Split into train and test data sets. Test data will be set aside and not used until the final model is tested.

```{r create-data-splits}
data_split <- initial_split(df, strata = category)
data_train <- training(data_split)
data_test <- testing(data_split)
```

Verify train/test splits have same proportions as original data set. Check.

```{r verify-splits}
# training set proportions by category
data_train %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))

# test set proportions by category
data_test  %>% 
  count(category) %>% 
  mutate(prop = n/sum(n))
```

## Resampling for evaluating performance

To evaluate model performance during during model selection we will use 10-fold cross-validation. Let's define the folds and make sure folds have same strata as original data.

```{r folds-setup}
data_folds <- vfold_cv(data_train, strata = category) # defaults to 10 fold, 1 repeats
data_folds
```

## Recipe

Set up model recipe to predict **category** (outcome) from the following predictors - email **subject**, sender email address (**from**), and the email **text**.  

Predictors are tokenized, stop words are removed, tokens are stemmed, and filtered (max number of tokens retained and min times a token must appear in corpus in order to be considered). Finally, tokens are encoded using **tf-idf**. 

Knowing that we have a large quantity of data but the data is unbalanced, we downsample such that the majority class level will have (at most) twice as many rows as the minority level.  

Token transformations are done via [textrecipes] (https://www.rdocumentation.org/packages/textrecipes/versions/1.0.10)

Most models work with numeric data, not text. Text data is typically converted into a numeric representation before it can be used to build models. There are a number of techniques for encoding text such as  [id-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [word embedding](https://www.tensorflow.org/text/guide/word_embeddings). Both techniques can be used with tidymodels.

Load Stanford GloVE(https://nlp.stanford.edu/pubs/glove.pdf) trained on words from a 6 billion token corpus.

```{r}
glove6b <- textdata::embedding_glove6b(dimensions = 100)
glove6b
```

Each token (word) is represented by a 100 dimension vector. Let's take a look at the vector for the first word 'hello'.

```{r}
glove6b %>% filter(token == 'hello')
```

We will use work embedding in the data recipe pipeline to encode predictor variables.

```{r create-model-recipe}
library(themis) # for step_downsample
data_recipe <- recipe(category ~ ., data = data_train)
data_recipe <- data_recipe %>%
                # retain these variables for reference (not to be used as model outcome or predictors)
                update_role(id, email_src, n_words, new_role = "REF") %>%
                step_tokenize(all_predictors(), 
                                options = list(lowercase = TRUE, 
                                strip_punct = TRUE)) %>%
                
                # set max tokens (words) for each predictor
                step_tokenfilter(all_predictors(), max_tokens = 200) %>%
    
                # remove stopwords (optional)
                step_stopwords(text) %>%
                step_stopwords(subject) %>%

                # stemming (optional)
                step_stem(text) %>%
                step_stem(subject) %>%

                # merge from and subject (optional)
                step_tokenmerge(from, subject) %>%
    
                # pretrained Glove word embedding
                step_word_embeddings(all_predictors(), embeddings = glove6b) %>%
                
                # tf-idf encoding is anothr option
                # step_tfidf(all_predictors()) %>% #TF-IDF word embedding

                step_downsample(category, 
                                under_ratio = 2 
                                )
data_recipe
```

Set up metrics of interest.

```{r metrix-setup}
metrics = metric_set(accuracy, roc_auc, pr_auc, precision, recall)
metrics
```

Set up re-sampling control.

```{r control-setup}
control <- control_resamples(save_pred = TRUE, allow_par = TRUE, parallel_over = "resamples")
control
```

## Null model

It's important to look at the **null model** to set up a baseline. For binary classification modeling, the null model always predicts the majority class. Consequently, **accuracy** of null model equals proportion of the majority class. For example, if there are 90% ham (not spam) observations then accuracy of the null model will also be 90%. Another way to look at it is that a model that is 90% accurate is not necessarily any good!

```{r cross-validate-null-model timeit=TRUE}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(null_classification) %>%
  fit_resamples(
    data_folds, 
    metrics = metrics,
    control = control
  )
```

We can see the null model has Accuracy equal to proportion of the majority class and a terrible [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Slope of the ROC curve shows the null model is unable to discriminate between the two classes.

```{r roc-plot-null-model}
null_predictions <- collect_predictions(null_rs)
null_predictions  %>%
  roc_curve(truth = category, .pred_spam) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (spam)",
    subtitle = "With null classifier on the training set"
  )
null_rs %>%
  collect_metrics()
```

The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) confirms that the null model simply guesses the majority class *ham* for each prediction. 

```{r confusion-matrix-null-model}
conf_mat_resampled(null_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")

```

## Model setup

The [tidymodels](https://tidymodels.tidymodels.org) framework makes it easy to plug-and-play with models. Below are some models that we can use - un-comment to select.

```{r models}

# Randomforest
model <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine(engine = "ranger", num.threads = usable_cores, importance = "impurity")


#Linear support vector machine.
# model <- svm_linear() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "kernlab")

# Boosted trees.
# model <- boost_tree() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "xgboost", trees = 20)
# model

# Linear support vector machine.
# model <- svm_linear() %>%
#   set_mode("classification") %>%
#   set_engine(engine = "kernlab")
# model

# Multi-layer perceptron, a single layer neural network.
# num_epochs = 50 # for SMS data set
# num_epochs = 50 # for Email data set
# model <- mlp(hidden_units = 128, dropout = 0.1, epochs = num_epochs) %>%
#   set_mode("classification") %>%
#   # Also set engine-specific `verbose` argument to prevent logging the results:
#   set_engine("keras", verbose = 1)

model
```

## Workflow setup

Create [workflow](https://workflows.tidymodels.org) to wrap data recipe and model.

```{r setup-workflow}
data_wf <- workflow() %>%
                add_recipe(data_recipe) %>%
                add_model(model)
data_wf
```

## Model training

Train the model on folds, record desired metrics, save predictions.

```{r cross-validate-model timeit=TRUE}
nb_rs <- fit_resamples(
    data_wf,
    data_folds,
    metrics = metrics,
    control = control
)
```

## Model evaluation

Extract metrics. Metrics look great across the board! In particular, the precision/recall and ROC metrics are very encouraging.

```{r eval-metrics}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_metrics
```

Plot ROC curve for all folds.

```{r eval-roc-plot}
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = category, .pred_spam) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "Each resample fold is shown in a different color"
  )
```

Confusion matrix confirms the model performance is excellent.

```{r eval-confusion-matrix}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
```

## Final model

So far we have been using the training data to both train and evaluate the model (using cross-validation). We can expect that the model **will not** perform quite as well on unseen data.  

So far we have been keeping the test split in our back pocket. Now it's time to use the precious test data to evaluate the final model. Do final fit - train with full training data and test with testing data.

```{r final-fit-model timeit=TRUE}
final_fitted <- last_fit(data_wf, data_split, metrics = metrics)
final_fitted
```

Extract predictions and metrics. The model has performed extremely well acrss all metrics. 

```{r}
final_predictions <- collect_predictions(final_fitted)
final_metrics <- collect_metrics(final_fitted)
final_metrics
```

The confusion matrix confirms the model is performing well.

```{r}
final_predictions %>%
  conf_mat(truth = category, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Plot final model's ROC curve.

```{r}
final_predictions  %>%
  roc_curve(truth = category, .pred_spam) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for observations (ham)",
    subtitle = "With final random forest classifier on the training set"
  )
```

