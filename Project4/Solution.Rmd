---
title: "Project 4"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  pdf_document: 
    toc: true
    number_sections: true
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libs}
library(tidyverse)
library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(caret)
```

```{r set-seed}
set.seed(1234)

```

```{r load-data}
sms <- read.delim("SMSSpamCollection.txt", 
                  sep = '\t', 
                  col.names = c('cat', 'sms'), 
                  quote = "")
```


```{r create-corpus}
corp_sms <- corpus(sms, 
                   text_field = "sms")
```


```{r}
# Add numeric document id, used later to partition train/test sets
corp_sms$id_numeric <- 1:ndoc(corp_sms)

# tokenize
toks_sms <- tokens(corp_sms, 
                remove_punct = TRUE, 
                remove_numbers = TRUE,
                remove_symbols = TRUE, 
                split_hyphens = TRUE) %>%
            tokens_remove(pattern = c(stopwords("en"), "lt", "gt"),
                        valuetype = "fixed",
                        padding = FALSE, 
                        min_nchar = 2) %>%
            tokens_tolower(keep_acronyms = TRUE) %>%
            tokens_wordstem()

# doc frequency matrix
dfm_sms <- dfm(toks_sms)
dfm_sms <- dfm_trim(dfm_sms, 
                    min_termfreq = 50)
topfeatures(dfm_sms)
```


```{r}
textplot_wordcloud(dfm_sms, 
                   max_words = 50, 
                   rotation = 0.1, 
                   color = "darkred")
```


```{r}
fcm_sms <- fcm(dfm_sms, 
               context = "document", 
               count = "frequency", 
               window = 5L)

topfeatures(fcm_sms, 
            n = 20, 
            scheme = "docfreq")
```


```{r}
feat <- names(topfeatures(fcm_sms, 
                          n = 20, 
                          scheme = "docfreq"))

fcm_sms_select = fcm_select(fcm_sms, 
                            pattern = feat, 
                            selection = "keep")

size <- log(colSums(dfm_select(dfm_sms, 
                               feat, 
                               selection = "keep")))

textplot_network(fcm_sms_select, 
                 min_freq = 0.8, 
                 vertex_size = size / max(size) * 3)
```


```{r}
corp_sms_sz <- length(docnames(corp_sms))

# train with 75% data (25% reserved for testing)
id_train <- sample(1:corp_sms_sz, corp_sms_sz * 0.75, replace = FALSE)
head(id_train, 10)
```


```{r}
dfm_training <- dfm_subset(dfm_sms, id_numeric %in% id_train)
dfm_test <- dfm_subset(dfm_sms, !id_numeric %in% id_train)
```


```{r}

# Naive Bayes classifier for texts
tmod_nb <- textmodel_nb(dfm_training, 
                        dfm_training$cat, 
                        prior = "docfreq") # uniform, docfreq, termfreq
summary(tmod_nb)
```


```{r}
dfm_matched <- dfm_match(dfm_test, features = featnames(dfm_training))
```


```{r}
actual_class <- dfm_matched$cat
predicted_class <- predict(tmod_nb, newdata = dfm_matched)
tab_class <- table(actual_class, predicted_class)
tab_class
```

Sensitivity (True positive rate) = (True Positive)/(True Positive + False Negative)
Specificity = (True Negative)/(True Negative + False Positive)

```{r}
confusionMatrix(tab_class, mode = "everything", positive = "spam")

```

